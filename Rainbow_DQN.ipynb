{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainbow DQN Implementation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Hello, here is my implementation of the algorithm  [Rainbow DQN](https://arxiv.org/pdf/1710.02298), algorithm published by Deepmind in October 2017.\n",
    "\n",
    "__Note__: I didn't implement some features (like Noisy Nets) because I didn't need it for the environments I used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now let's go to the implementation !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: box2d in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (2.3.2)\n",
      "Requirement already satisfied: box2d-kengz in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (2.3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pygame in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (2.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (0.29.1)\n",
      "Requirement already satisfied: pygame in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (from gymnasium) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/rahul25/.conda/envs/rahulenv/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Run me :D\n",
    "%pip install numpy torch torchvision gymnasium tqdm einops\n",
    "%pip install gymnasium[box2d]\n",
    "%pip install box2d box2d-kengz\n",
    "%pip install pygame\n",
    "%pip install --upgrade gymnasium pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again... Don't think, RUN !\n",
    "import numpy as np\n",
    "import einops\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ReplayBuffer import ReplayBuffer, ALPHA\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some constants used during the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants:\n",
    "EPISODES = 500\n",
    "FRAME = 1000\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR=5e-4\n",
    "\n",
    "MEMORY_CAPACITY = 100\n",
    "\n",
    "# Epsilon parameters\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.01\n",
    "\n",
    "UPDATE_FREQUENCY = 1000\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the Neural Network needed that we implement following the dueling architecture.\n",
    "\n",
    "__NOTE__: Q(s, a) = V(s) + A(s, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        # State Value Stream\n",
    "        self.state_nn = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "        # Advantage Stream\n",
    "        self.advantage_nn = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, env.action_space.n),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        # State Value prediction\n",
    "        state_value = self.state_nn(x)\n",
    "\n",
    "        # Advantage prediction\n",
    "        advantage = self.advantage_nn(x)\n",
    "        # Getting the mean of advantage (we could use the max also)\n",
    "        average_advantage = advantage.mean()\n",
    "        # Substracting the mean to the advantage to not have too much relative advantages\n",
    "        return state_value + (advantage - average_advantage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils functions for training process:\n",
    "*   For the Gradient Descent\n",
    "*   The copy of the online network param into the target's one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(loss: torch.Tensor, optimizer: torch.optim.Optimizer) -> None:\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "    # Backpropagate\n",
    "    loss.backward()\n",
    "    # Apply gradient on the Neural Network\n",
    "    optimizer.step()\n",
    "\n",
    "def copy_nn_parameters(target: nn.Module, online: nn.Module) -> None:\n",
    "    target.load_state_dict(online.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the Rainbow Agent class, agents that will make the actions.\n",
    "\n",
    "We also initialize the `ReplayBuffer` useful for the training process with a `fill_buffer` method\n",
    "that will fill the `ReplayBuffer` with a certain amount of episodes based on the `MEMORY_CAPACITY` constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowAgent:\n",
    "\n",
    "    def __init__(self, N) -> None:\n",
    "        self.buffer = ReplayBuffer(N)\n",
    "\n",
    "    def fill_buffer(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "    ) -> None:\n",
    "        state, _ = env.reset()\n",
    "        for _ in tqdm(range(MEMORY_CAPACITY)):\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            experience = (state, action, reward, done, new_state)\n",
    "            self.buffer.init_transition(experience=experience, priority=10e-5, proba=(1 / MEMORY_CAPACITY), weight=1.0)\n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                state, _ = env.reset()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_action(\n",
    "        state: np.ndarray,\n",
    "        env: gym.Env,\n",
    "        epsilon: float,\n",
    "        network: nn.Module\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "            Given a `state`, an `env`, an `epsilon` value and a `network` compute the following action,\n",
    "            with respect on the current policy.\n",
    "        \"\"\"\n",
    "\n",
    "        states = torch.as_tensor(state, dtype=torch.float32)\n",
    "        q_values: torch.Tensor = network.forward(states)\n",
    "        greedy_action: int = q_values.argmax().item()\n",
    "        action = RainbowAgent.epsilon_greedy_policy(\n",
    "            env,\n",
    "            epsilon,\n",
    "            greedy_action\n",
    "        )\n",
    "        return action\n",
    "\n",
    "    @staticmethod\n",
    "    def epsilon_greedy_policy(\n",
    "        env: gym.Env,\n",
    "        epsilon: float,\n",
    "        greedy_action: int\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "            Implementation of the Epsilon greedy policy.\n",
    "        \"\"\"\n",
    "\n",
    "        bool_action = random.random() > epsilon\n",
    "        if bool_action:\n",
    "            action = greedy_action\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of all the things we need. (Network, agent, environment, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment:\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Load classes needed:\n",
    "Rainbow = RainbowAgent(N=MEMORY_CAPACITY)\n",
    "online_network = RainbowNetwork(env).to(device=device)\n",
    "target_network = RainbowNetwork(env).to(device=device)\n",
    "\n",
    "# Get the optimizer and the loss_fn:\n",
    "optimizer = torch.optim.RMSprop(online_network.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.SmoothL1Loss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are training our Rainbow DQN with the environment initialized before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rainbow(\n",
    "    env: gym.Env,\n",
    "    rainbow: RainbowAgent,\n",
    "    online_network: RainbowNetwork,\n",
    "    target_network: RainbowNetwork,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "        This function is used to train the Rainbow DQN model with the 2 networks, the agent and the `ReplayBuffer`.\n",
    "\n",
    "        NOTE: The env has to be opened before and be closed after the return of this function.\n",
    "    \"\"\"\n",
    "    epsilon = EPS_START\n",
    "    steps = 0\n",
    "\n",
    "    for episode in tqdm(range(1, EPISODES + 1)):\n",
    "        rewards = []\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        ### Here we have some epsilon decay options ###\n",
    "        # epsilon = max(0.1, epsilon - 1.0 / EPISODES * 2)\n",
    "        epsilon = max(epsilon * np.exp(-EPS_DECAY), EPS_END)\n",
    "        # BETA = 0.4 + (1.0 - 0.4) * (episode / EPISODES)\n",
    "\n",
    "        for _ in range(1, FRAME + 1):\n",
    "            ### Here we have some epsilon decay options ###\n",
    "            # epsilon = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps / EPS_DECAY)\n",
    "            # epsilon = max(0.1, epsilon * 0.999) # decay factor of epsilon\n",
    "\n",
    "            # Taking an action\n",
    "            action = rainbow.get_action(state, env, epsilon, online_network)\n",
    "\n",
    "            # Observe env return values\n",
    "            new_state, reward, termination, truncation, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # Add the experience to the experience relay\n",
    "            rainbow.buffer.store_transition((state, action, reward, termination, new_state))\n",
    "\n",
    "            # Go to next state\n",
    "            state = new_state\n",
    "            # Sample an experience and get its state, action, reward and new_state\n",
    "            state_t, action_t, reward_t, termination_t, new_state_t, weights, indexes = rainbow.buffer.retrieve_transitions()\n",
    "\n",
    "            # Implement loss\n",
    "            max_next_state = target_network.forward(new_state_t).max(dim=1, keepdim=True)[0]\n",
    "            y = reward_t + (GAMMA * max_next_state * (1 - termination_t))\n",
    "\n",
    "            current_q = online_network.forward(state_t).gather(dim=1, index=action_t)\n",
    "\n",
    "            error = loss_fn(y, current_q)\n",
    "            loss = torch.mean(error * weights)\n",
    "\n",
    "            # Gradient Descent on loss with respect with the optimizer\n",
    "            gradient_descent(loss, optimizer)\n",
    "\n",
    "            # Increment step at each gradient descent\n",
    "            steps += 1\n",
    "\n",
    "            new_priorities = error ** ALPHA\n",
    "            rainbow.buffer.update_priority(new_priorities=new_priorities, indexes=indexes)\n",
    "\n",
    "            if steps % UPDATE_FREQUENCY == 0:\n",
    "                copy_nn_parameters(target_network, online_network)\n",
    "\n",
    "            # If it is the end pass to the next episode\n",
    "            if termination or truncation:\n",
    "                break\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"We are at episode {episode}\\t\\\n",
    "                mean reward is {np.mean(rewards):.3f}\\\n",
    "                and sum reward is {sum(rewards):.3f}\\t\\\n",
    "                epsilon is {epsilon:.2f}\"\n",
    "            )\n",
    "            if sum(rewards) >= 200:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rainbow.fill_buffer(env)\n",
    "train_rainbow(env, Rainbow, online_network, target_network, loss_fn, optimizer)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are few testing functions for the Rainbow DQN. (Choose the one that work the most in your conda environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rainbow_plt(\n",
    "    env: gym.Env,\n",
    "    rainbow: RainbowAgent,\n",
    "    network: RainbowNetwork\n",
    "):\n",
    "    \"\"\"\n",
    "        * `env`: Gymnasium environment that MUST be in 'rgb_array' render_mode\n",
    "        * `rainbow`: The Rainbow Agent that choose the action\n",
    "        * `network`: The Rainbow Network that perform the prediction of future actions\n",
    "\n",
    "        NOTE: Here we use plt to display, use this if you can't run in render_mode.\n",
    "    \"\"\"\n",
    "    epsilon = 0.0\n",
    "    for episode in range(1, 6):\n",
    "        rewards = []\n",
    "        state, _ = env.reset()\n",
    "        for _ in range(1, 1000):\n",
    "            # Taking an action\n",
    "            action = rainbow.get_action(state, env, epsilon, network)\n",
    "    \n",
    "            # Observe env return values\n",
    "            new_state, reward, termination, truncation, _ = env.step(action)\n",
    "    \n",
    "            # Go to the next state for next action\n",
    "            state = new_state\n",
    "    \n",
    "            # Get all the rewards\n",
    "            rewards.append(reward)\n",
    "    \n",
    "            # If it is the end pass to the next episode\n",
    "            if termination or truncation:\n",
    "                break\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(env.render())\n",
    "            plt.show()\n",
    " \n",
    "        # Print the mean recent reward every 50 episodes\n",
    "        if episode % 5 == 0:\n",
    "            print(f\"Episode {episode:>6}: \\tR:{np.mean(rewards):>6.3f}\")\n",
    "\n",
    "def test_rainbow(\n",
    "    env: gym.Env,\n",
    "    rainbow: RainbowAgent,\n",
    "    network: RainbowNetwork\n",
    "):\n",
    "    \"\"\"\n",
    "        * `env`: Gymnasium environment that MUST be in 'human' render_mode\n",
    "        * `rainbow`: The Rainbow Agent that choose the action\n",
    "        * `network`: The Rainbow Network that perform the prediction of future actions\n",
    "    \"\"\"\n",
    "    epsilon = 0.0\n",
    "    for episode in range(1, 6):\n",
    "        rewards = []\n",
    "        state, _ = env.reset()\n",
    "        for _ in range(1, 1000):\n",
    "            # Taking an action\n",
    "            action = rainbow.get_action(state, env, epsilon, network)\n",
    "    \n",
    "            # Observe env return values\n",
    "            new_state, reward, termination, truncation, _ = env.step(action)\n",
    "    \n",
    "            # Go to the next state for next action\n",
    "            state = new_state\n",
    "    \n",
    "            # Get all the rewards\n",
    "            rewards.append(reward)\n",
    "    \n",
    "            # If it is the end pass to the next episode\n",
    "            if termination or truncation:\n",
    "                break\n",
    " \n",
    "        # Print the mean recent reward every 50 episodes\n",
    "        if episode % 5 == 0:\n",
    "            print(f\"Episode {episode:>6}: \\tR:{np.mean(rewards):>6.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()\n",
    "# Let's compute the real deal !\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "test_rainbow_plt(env, Rainbow, online_network)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
